{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Imports and constants","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.utils.data as thd\nimport torch.nn as nn\n\nfrom sklearn.metrics import fbeta_score, precision_score, recall_score\nfrom scipy.ndimage.filters import gaussian_filter1d\n\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport os\nimport gc\nfrom operator import itemgetter\nfrom pympler import tracker\n\n# Ignore SKLearn warnings\nimport warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\nwarnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n\nBATCH_SIZE = 256\nBUFFER = 10  # Buffer size in both dimensions: x and y. Effective patch size is [BUFFER * 2 + 1, BUFFER * 2 + 1, Z_DIM].\nSLICES = 65\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLEARNING_RATE = 0.3\nZ_START = 24\nZ_END = 34\nZ_STEP = 1\nZ_DIM = (Z_END - Z_START) // Z_STEP\nTRAIN_ON_FRAGMENTS = [1, 2]\nVAL_FRAGMENT = 3\nDISABLE_TQDM = False\n\nMAX_TRAIN_STEPS = 300000000\nMAX_VAL_STEPS = 1000\nPRINT_EVERY = 20000000","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-30T11:46:27.946286Z","iopub.execute_input":"2023-05-30T11:46:27.946555Z","iopub.status.idle":"2023-05-30T11:46:33.007883Z","shell.execute_reply.started":"2023-05-30T11:46:27.946531Z","shell.execute_reply":"2023-05-30T11:46:33.006635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, let's see if we can fit all one full fragment into memory at once.","metadata":{}},{"cell_type":"code","source":"def pad_array(array):\n    padding = (\n        (BUFFER, BUFFER),\n        (BUFFER, BUFFER),\n    )\n    return np.pad(array, padding)\n\ndef load_fragment_surface(fragment, split='train'):\n    print(\"Loading fragment %s surface\" % fragment)\n    surface_path = Path(\"/kaggle/input/vesuvius-challenge-ink-detection/%s/%s/surface_volume\" % (split, fragment))\n    return np.array([\n        (pad_array(np.array(Image.open(f))) / (2 ** 16)).astype('float16')\n        for f in tqdm(sorted(surface_path.rglob(\"*.tif\"))[Z_START:Z_END:Z_STEP], disable=DISABLE_TQDM)\n    ])\n\ndef load_mask(fragment, split='train'):\n    print(\"Loading fragment %s mask\" % fragment)\n    mask_path = Path(\"/kaggle/input/vesuvius-challenge-ink-detection/%s/%s/mask.png\" % (split, fragment))\n    return pad_array(np.array(Image.open(mask_path)))\n\ndef load_inklabels(fragment, split='train'):\n    print(\"Loading fragment %s labels\" % fragment)\n    inklabels_path = Path(\"/kaggle/input/vesuvius-challenge-ink-detection/%s/%s/inklabels.png\" % (split, fragment))\n    return pad_array(np.array(Image.open(inklabels_path)))","metadata":{"execution":{"iopub.status.busy":"2023-05-30T11:46:33.010020Z","iopub.execute_input":"2023-05-30T11:46:33.014386Z","iopub.status.idle":"2023-05-30T11:46:33.028408Z","shell.execute_reply.started":"2023-05-30T11:46:33.014348Z","shell.execute_reply":"2023-05-30T11:46:33.027587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SingleFragmentDataset(thd.Dataset):\n    def __init__(self, fragment, split_name='train'):\n        assert split_name in ['train', 'val', 'test']\n        self.split_name = split_name\n        directory = 'test' if split_name == 'test' else 'train'\n        self.surface = load_fragment_surface(fragment, directory)\n        print(self.surface.dtype)\n        self.mask = load_mask(fragment, directory)\n        self.inklabels = load_inklabels(fragment, directory) if self.split_name != 'test' else None\n        self.pixels = np.stack(np.where(self.mask == 1), axis=1)\n    \n    def __len__(self):\n        return self.pixels.shape[0]\n    \n    def get_pixel_number(self, y, x):\n        return 1 + (y - BUFFER) * (self.surface.shape[2] - 2 * BUFFER) + (x - BUFFER)\n    \n    def __getitem__(self, index):\n        y, x = self.pixels[index]\n        y_start = y - BUFFER\n        y_end = y + BUFFER + 1\n        x_start = x - BUFFER\n        x_end = x + BUFFER + 1\n        patch_surface = np.s_[:, y_start:y_end, x_start:x_end]\n        patch_labels = np.s_[y_start:y_end, x_start:x_end]\n        surface = self.surface[patch_surface].astype(np.float32)\n        labels = self.inklabels[y, x].reshape((1, )).astype(np.float32) if self.split_name != 'test' else None\n        pixel_number = self.get_pixel_number(y, x)\n        return {\n            'train': (surface, labels),\n            'val': (surface, labels, pixel_number),\n            'test': (surface, pixel_number)\n        }[self.split_name]","metadata":{"execution":{"iopub.status.busy":"2023-05-30T11:46:33.029914Z","iopub.execute_input":"2023-05-30T11:46:33.030536Z","iopub.status.idle":"2023-05-30T11:46:33.049078Z","shell.execute_reply.started":"2023-05-30T11:46:33.030502Z","shell.execute_reply":"2023-05-30T11:46:33.048116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the model","metadata":{}},{"cell_type":"code","source":"convnet = nn.Sequential(\n    nn.Conv2d(Z_DIM, 32, kernel_size=3, stride=1, dilation=1, padding='same'),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.BatchNorm2d(32),\n    \n    nn.Conv2d(32, 32, kernel_size=3, stride=1, dilation=1, padding='same'),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.BatchNorm2d(32),\n    \n    nn.Conv2d(32, 32, kernel_size=3, stride=1, dilation=1, padding='same'),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.BatchNorm2d(32),\n    \n    nn.Conv2d(32, 32, kernel_size=3, stride=1, dilation=1, padding='same'),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.BatchNorm2d(32),\n    \n    nn.Conv2d(32, 32, kernel_size=3, stride=1, dilation=1, padding='same'),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.BatchNorm2d(32),\n    \n    nn.Conv2d(32, 1, kernel_size=3, stride=1, dilation=1, padding='same'),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear((2 * BUFFER + 1) ** 2, 128),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.Linear(128, 64),\n    nn.Dropout(p=0.2),\n    nn.ReLU(),\n    nn.Linear(64, 1),\n    nn.Sigmoid()\n).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T11:46:33.051750Z","iopub.execute_input":"2023-05-30T11:46:33.052377Z","iopub.status.idle":"2023-05-30T11:46:36.021394Z","shell.execute_reply.started":"2023-05-30T11:46:33.052325Z","shell.execute_reply":"2023-05-30T11:46:36.020359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model","metadata":{}},{"cell_type":"code","source":"%%time\n\ncriterion = nn.BCELoss()\n\nfig, axs = plt.subplots(2)\nfig.suptitle('Training')\n\ndef train(fragment_number, pltaxs):\n    train_dataset = SingleFragmentDataset(fragment_number)\n    print(train_dataset.surface.shape)\n    train_loader = thd.DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n    \n    print(gc.collect())\n    mem = tracker.SummaryTracker()\n    print(sorted(mem.create_summary(), reverse=True, key=itemgetter(2))[:10])\n    \n    losses = []\n    fbetas = []\n    precisions = []\n    recalls = []\n\n    convnet.train()\n    optimizer = torch.optim.SGD(convnet.parameters(), lr=LEARNING_RATE)\n    for i, (xs, ys) in enumerate(pbar := tqdm(train_loader, disable=DISABLE_TQDM)):\n        if i > MAX_TRAIN_STEPS:\n            break\n        optimizer.zero_grad()\n        outputs = convnet(xs.to(DEVICE))\n        loss = criterion(outputs, ys.to(DEVICE))\n        pred_ink = outputs.detach().gt(0.4).cpu().int()\n        fbeta = fbeta_score(ys.view(-1).numpy(), pred_ink.view(-1).numpy(), beta=0.5)\n        precision = precision_score(ys.view(-1).numpy(), pred_ink.view(-1).numpy())\n        recall = recall_score(ys.view(-1).numpy(), pred_ink.view(-1).numpy())\n        pbar.set_postfix({\"loss\": loss, \"prec\": precision, \"rec\": recall, \"fbeta\": fbeta})\n        loss.backward()\n        optimizer.step()\n\n        fbetas.append(fbeta)\n        losses.append(loss.detach().cpu().float())\n        precisions.append(precision)\n        recalls.append(recall)\n    \n    del train_loader\n    del train_dataset\n    gc.collect()\n    \n    pltaxs.plot(gaussian_filter1d(losses, sigma=25), label='loss')\n    pltaxs.plot(gaussian_filter1d(precisions, sigma=25), label='precisions')\n    pltaxs.plot(gaussian_filter1d(recalls, sigma=25), label='recalls')\n    pltaxs.plot(gaussian_filter1d(fbetas, sigma=25), label='fbetas')\n    pltaxs.legend()\n\nfor i, fragment_number in enumerate(TRAIN_ON_FRAGMENTS):\n    train(fragment_number, axs[i])","metadata":{"execution":{"iopub.status.busy":"2023-05-30T11:40:17.166408Z","iopub.execute_input":"2023-05-30T11:40:17.167133Z","iopub.status.idle":"2023-05-30T11:44:19.161879Z","shell.execute_reply.started":"2023-05-30T11:40:17.167097Z","shell.execute_reply":"2023-05-30T11:44:19.159731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pick the best threshold according to validation set","metadata":{}},{"cell_type":"code","source":"convnet.eval()\nval_dataset = SingleFragmentDataset(VAL_FRAGMENT, split_name='val')\nval_loader = thd.DataLoader(val_dataset, BATCH_SIZE, shuffle=True)\n\ndef evaluate(threshold):\n    print(\"Evaluating for threshold %f\" % threshold)\n    losses = []\n    accs = []\n    fbetas = []\n    precisions = []\n    recalls = []\n    \n    img = np.zeros(val_dataset.surface.shape[1:])\n\n    for i, (xs, ys, pixels) in enumerate(pbar := tqdm(val_loader, disable=DISABLE_TQDM)):\n        if i > MAX_VAL_STEPS:\n            break\n        outputs = convnet(xs.to(DEVICE))\n        loss = criterion(outputs, ys.to(DEVICE))\n        pred_ink = outputs.detach().gt(threshold).cpu().int()\n        accuracy = (pred_ink == ys).sum().float().div(ys.size(0))\n        fbeta = fbeta_score(ys.view(-1).numpy(), pred_ink.view(-1).numpy(), beta=0.5)\n        precision = precision_score(ys.view(-1).numpy(), pred_ink.view(-1).numpy())\n        recall = recall_score(ys.view(-1).numpy(), pred_ink.view(-1).numpy())\n        pbar.set_postfix({\"loss\": loss, \"acc\": accuracy, \"fbeta\": fbeta})\n\n        fbetas.append(fbeta)\n        losses.append(loss.detach().cpu().float())\n        accs.append(accuracy)\n        precisions.append(precision)\n        recalls.append(recall)\n        \n        pixels_with_ink = ys[pred_ink == 1].int().tolist()\n        for p in pixels_with_ink:\n            img[p] = 1\n    \n    plt.imshow(img)\n    plt.show()\n    \n    return np.mean(losses), np.mean(accs), np.mean(fbetas), np.mean(precisions), np.mean(recalls)\n\n\ndef evaluate_for_thresholds():\n    thresholds = [0.1, 0.125, 0.15, 0.175, 0.2, 0.25, 0.325, 0.4]\n    \n    best_fbeta = 0.\n    best_threshold = 0\n    \n    fbetas = []\n    precisions = []\n    recalls = []\n\n    for threshold in thresholds:\n        _, _, fbeta, precision, recall = evaluate(threshold)\n        if fbeta > best_fbeta:\n            best_fbeta = fbeta\n            best_threshold = threshold\n        fbetas.append(fbeta)\n        precisions.append(precision)\n        recalls.append(recall)\n    \n    plt.plot(thresholds, fbetas, label=\"fbeta\")\n    plt.plot(thresholds, precisions, label=\"precision\")\n    plt.plot(thresholds, recalls, label=\"recall\")\n    plt.legend()\n    \n    return best_threshold\n\nbest_threshold = evaluate_for_thresholds()\nprint(\"Best threshold:\", best_threshold)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T11:50:33.911823Z","iopub.execute_input":"2023-05-30T11:50:33.912483Z","iopub.status.idle":"2023-05-30T11:50:35.499275Z","shell.execute_reply.started":"2023-05-30T11:50:33.912447Z","shell.execute_reply":"2023-05-30T11:50:35.497920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Validate on full validation set","metadata":{}},{"cell_type":"code","source":"# MAX_VAL_STEPS = 10**10\n# evaluate(best_threshold)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate test predictions","metadata":{}},{"cell_type":"code","source":"# [0, 1, 2, 4, 5]\n# [-INF, 0, 1, 2, 4, 5, INF]\n# [(0, -INF), (1, 0), (2, 1), (4, 2), (5, 4), (INF, 5)]\n# starts: [0, 4, INF]\n# ends: [-INF, 2, 5]\n\ndef rle(sorted_pixels):\n    print(\"Encoding RLE\")\n    INF = 10**15\n    sorted_pixels = [-INF] + sorted_pixels + [INF]\n    zipped = list(zip(sorted_pixels[1:], sorted_pixels[:-1]))\n    starts = [p[0] for p in zipped if p[0] != p[1] + 1]\n    ends = [p[1] for p in zipped if p[0] != p[1] + 1]\n    pairs = [(p[0], p[1] - p[0] + 1) for p in zip(starts[:-1], ends[1:])]\n    return ' '.join(list(map(lambda p: \"%d %d\" % (p[0], p[1]), pairs)))\n\nprint(rle([0, 1, 2, 4, 5]))\nprint(rle([]))\nprint(rle([1, 3, 5, 7]))\n# assert rle([0, 1, 2, 4, 5]) == \"0 3 4 2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del val_loader\ndel val_dataset\ngc.collect()\n\nsubmission = defaultdict(list)\n\nfor fragment in ['a', 'b']:\n    pixels_with_ink = []\n    print(f\"Generating predictions for fragment {fragment}\")\n    test_dataset = SingleFragmentDataset(fragment, split_name='test')\n    test_loader = thd.DataLoader(test_dataset, BATCH_SIZE, shuffle=True)\n    for (xs, ys) in (pbar := tqdm(test_loader, disable=DISABLE_TQDM)):\n        outputs = convnet(xs.to(DEVICE))\n        pred_ink = outputs.detach().gt(best_threshold).flatten().cpu().int()\n        pred_ink = pred_ink[:len(ys)]\n        pixels_with_ink += ys[pred_ink == 1].int().tolist()\n    pixels_with_ink.sort()\n    \n    submission[\"Id\"].append(fragment)\n    submission[\"Predicted\"].append(rle(pixels_with_ink))\n    \npd.DataFrame.from_dict(submission).to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame.from_dict(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}